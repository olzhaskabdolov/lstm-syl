{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global hyperparameters\n",
    "batch_size = 20\n",
    "max_grad_norm = 5\n",
    "lr_decay = 0.5\n",
    "learning_rate = 1.0\n",
    "init_scale = 0.05\n",
    "\n",
    "# LSTM hyperparameters\n",
    "num_steps = 35\n",
    "hidden_size = 300\n",
    "num_layers = 2\n",
    "keep_prob = 0.5\n",
    "\n",
    "# CNN hyperparameters\n",
    "char_emb_dim = 15\n",
    "filter_widths = list(range(1, 7))\n",
    "cnn_output_dim = sum([25 * w for w in filter_widths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# possibles languages: ptb, fr, es, de, cs, ru ('ptb' means English PTB dataset)\n",
    "lang = 'ru'\n",
    "\n",
    "if lang == 'ptb': \n",
    "  word_data = open('simple-examples/data/ptb.train.txt', 'r').read().replace('\\n', '<eos>').split()\n",
    "else:\n",
    "  word_data = open('data/'+lang+'/train.txt', 'r').read().replace('\\n', ' ').split()\n",
    "\n",
    "words = list(set(word_data))\n",
    "word_data_size, word_vocab_size = len(word_data), len(words)\n",
    "print('data has %d words, %d unique' % (word_data_size, word_vocab_size))\n",
    "\n",
    "word_to_ix = { word:i for i,word in enumerate(words) }\n",
    "ix_to_word = { i:word for i,word in enumerate(words) }\n",
    "\n",
    "def get_word_raw_data(input_file):\n",
    "  if lang == 'ptb':\n",
    "    data = open(input_file, 'r').read().replace('\\n', '<eos>').split()\n",
    "  else:\n",
    "    data = open(input_file, 'r').read().replace('\\n', ' ').split()\n",
    "  #return [word_to_ix[w] for w in data if w in word_to_ix]\n",
    "  return [word_to_ix[w] for w in data]\n",
    "\n",
    "train_raw_data = get_word_raw_data('data/'+lang+'/train.txt')\n",
    "valid_raw_data = get_word_raw_data('data/'+lang+'/valid.txt')\n",
    "test_raw_data = get_word_raw_data('data/'+lang+'/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = list(set(''.join(words)))\n",
    "char_vocab_size = len(chars)\n",
    "\n",
    "#three more characters: beginning and end of each word, zero-padding\n",
    "WORD_BEG_CHAR, WORD_END_CHAR, ZERO_PAD_CHAR = '⎡', '⎦', ' '\n",
    "chars.insert(0, ZERO_PAD_CHAR)\n",
    "chars.extend([WORD_BEG_CHAR, WORD_END_CHAR])\n",
    "char_vocab_size += 3\n",
    "max_word_len = max([ len(word) for word in words ]) + 2\n",
    "\n",
    "print('data has %d unique characters' % char_vocab_size)\n",
    "\n",
    "char_to_ix = { char:i for i,char in enumerate(chars) }\n",
    "ix_to_char = { i:char for i,char in enumerate(chars) }\n",
    "\n",
    "def word_ix_to_char_ixs(word_ix):\n",
    "  word = WORD_BEG_CHAR + ix_to_word[word_ix] + WORD_END_CHAR\n",
    "  word = word.ljust(max_word_len, ZERO_PAD_CHAR)\n",
    "  return [char_to_ix[c] for c in word]\n",
    "\n",
    "def word_ix_to_one_hot(word_ix):\n",
    "  result = np.zeros([word_vocab_size], dtype=np.float32)\n",
    "  result[word_ix] = 1.0\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class batch_producer(object):\n",
    "  def __init__(self, raw_data, batch_size, num_steps):\n",
    "    self.raw_data = raw_data\n",
    "    self.batch_size = batch_size\n",
    "    self.num_steps = num_steps\n",
    "    \n",
    "    self.batch_len = len(self.raw_data) // self.batch_size\n",
    "    self.data = np.reshape(self.raw_data[0 : self.batch_size * self.batch_len],\n",
    "                           (self.batch_size, self.batch_len))\n",
    "    \n",
    "    self.epoch_size = (self.batch_len - 1) // self.num_steps\n",
    "    self.i = 0\n",
    "  \n",
    "  def __next__(self):\n",
    "    if self.i < self.epoch_size:\n",
    "      # batch_x and batch_y are of shape [batch_size, num_steps]\n",
    "      batch_x = self.data[::, self.i * self.num_steps : (self.i + 1) * self.num_steps : ]\n",
    "      batch_y = self.data[::, self.i * self.num_steps + 1 : (self.i + 1) * self.num_steps + 1 : ]\n",
    "      self.i += 1\n",
    "      return (batch_x, batch_y)\n",
    "    else:\n",
    "      raise StopIteration()\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "  \n",
    "  char_embedding = tf.get_variable(\"char_embedding\", [char_vocab_size, char_emb_dim], dtype=tf.float32,\n",
    "                                  initializer=tf.random_uniform_initializer(-init_scale, init_scale))\n",
    "  \n",
    "  cell = tf.nn.rnn_cell.LSTMCell(hidden_size, forget_bias=0.0)\n",
    "  \n",
    "  def __init__(self, batch_size, need_reuse=False):\n",
    "    self.x = tf.placeholder(tf.int32, [batch_size, num_steps, max_word_len])\n",
    "    self.y = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    words_embedded = tf.nn.embedding_lookup(self.char_embedding, self.x)\n",
    "    words_embedded = tf.reshape(words_embedded, [-1, max_word_len, char_emb_dim])\n",
    "\n",
    "    def conv_layer(cur_char_inputs, filt_shape, bias_shape):\n",
    "      filt = tf.get_variable('filt', filt_shape, initializer=tf.random_uniform_initializer(-init_scale, init_scale))\n",
    "      bias = tf.get_variable('bias', bias_shape, initializer=tf.random_uniform_initializer(-init_scale, init_scale))\n",
    "      conv = tf.nn.conv1d(cur_char_inputs, filt, 1, padding='VALID')\n",
    "      feature_map = tf.nn.tanh(conv + bias)\n",
    "      feature_map_reshaped = tf.expand_dims(feature_map, 1)\n",
    "      pool = tf.nn.max_pool(feature_map_reshaped, [1, 1, max_word_len - filt_shape[0] + 1, 1], [1, 1, 1, 1], 'VALID')\n",
    "      return(tf.squeeze(pool, axis=[1,2]))\n",
    "\n",
    "    def words_filter(cur_char_inputs):\n",
    "      pools = []\n",
    "      for w in filter_widths:\n",
    "        with tf.variable_scope('filter' + str(w)):\n",
    "          #pools.append(conv_layer(cur_char_inputs, [w, char_emb_dim, min(200, w * 50)], [min(200, w * 50)]))\n",
    "          pools.append(conv_layer(cur_char_inputs, [w, char_emb_dim, w * 25], [w * 25]))\n",
    "      return tf.concat(1, pools)\n",
    "   \n",
    "    \n",
    "    with tf.variable_scope('cnn_output', reuse=need_reuse) as scope:\n",
    "      self.cnn_output = tf.reshape(words_filter(words_embedded), [-1, cnn_output_dim])\n",
    "  \n",
    "    with tf.variable_scope('highway', reuse=need_reuse):\n",
    "      transf_weights = tf.get_variable('transf_weights', [cnn_output_dim, cnn_output_dim],\n",
    "                                       initializer=tf.random_uniform_initializer(-init_scale, init_scale),\n",
    "                                       dtype=tf.float32)\n",
    "      transf_biases = tf.get_variable('transf_biases', [cnn_output_dim],\n",
    "                                     initializer=tf.random_uniform_initializer(-2-init_scale, -2+init_scale),\n",
    "                                     dtype=tf.float32)\n",
    "      highw_weights = tf.get_variable('highw_weights', [cnn_output_dim, cnn_output_dim],\n",
    "                                       initializer=tf.random_uniform_initializer(-init_scale, init_scale),\n",
    "                                       dtype=tf.float32)\n",
    "      highw_biases = tf.get_variable('highw_biases', [cnn_output_dim],\n",
    "                                     initializer=tf.random_uniform_initializer(-init_scale, init_scale),\n",
    "                                     dtype=tf.float32)\n",
    "      transf_gate = tf.nn.sigmoid(tf.matmul(self.cnn_output, transf_weights) + transf_biases)\n",
    "      highw_output = tf.mul(transf_gate, tf.nn.relu(tf.matmul(self.cnn_output, highw_weights) + highw_biases)) + \\\n",
    "                     tf.mul(tf.ones([cnn_output_dim], dtype=tf.float32) - transf_gate, self.cnn_output)\n",
    "      \n",
    "    highw_output_reshaped = tf.reshape(highw_output, [batch_size, num_steps, -1])\n",
    "    lstm_input = tf.unpack(highw_output_reshaped, axis=1)\n",
    "    \n",
    "    \n",
    "    self.apply_dropout()\n",
    "    self.cell = tf.nn.rnn_cell.MultiRNNCell([self.cell] * num_layers)\n",
    "    \n",
    "    self.init_state = self.cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    with tf.variable_scope('lstm_rnn', reuse=need_reuse):\n",
    "      outputs, self.state = tf.nn.rnn(self.cell, lstm_input, dtype=tf.float32, initial_state=self.init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax_params', reuse=need_reuse):\n",
    "      weights = tf.get_variable('weights', [hidden_size, word_vocab_size], \n",
    "                                initializer=tf.random_uniform_initializer(-init_scale, init_scale), \n",
    "                                dtype=tf.float32)\n",
    "      biases = tf.get_variable('biases', [word_vocab_size], \n",
    "                               initializer=tf.random_uniform_initializer(-init_scale, init_scale),\n",
    "                               dtype=tf.float32)\n",
    "\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, hidden_size])\n",
    "    logits = tf.matmul(output, weights) + biases\n",
    "    loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [tf.reshape(self.y, [-1])],\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "    self.cost = tf.reduce_sum(loss) / batch_size\n",
    "\n",
    "  \n",
    "  def apply_dropout(self):\n",
    "    #self.cnn_output = tf.nn.dropout(self.cnn_output, keep_prob=keep_prob)\n",
    "    self.cell = tf.nn.rnn_cell.DropoutWrapper(self.cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train(Model):\n",
    "  \n",
    "  def __init__(self, batch_size):\n",
    "    self.clear_char_embedding_padding = tf.scatter_update(self.char_embedding, \n",
    "                                                          [0], \n",
    "                                                          tf.constant(0.0, shape=[1, char_emb_dim], dtype=tf.float32)\n",
    "                                                         )\n",
    "  \n",
    "    super(Train, self).__init__(batch_size)\n",
    "    \n",
    "    self.lr = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "    self.train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                              global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "    \n",
    "    self.new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "    self.lr_update = tf.assign(self.lr, self.new_lr)\n",
    "\n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(self.lr_update, feed_dict={self.new_lr: lr_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Eval(Model):\n",
    "  \n",
    "  def apply_dropout(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = Train(batch_size)\n",
    "valid = Eval(batch_size, need_reuse=True)\n",
    "test = Eval(1, need_reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_size():\n",
    "  params = tf.trainable_variables()\n",
    "  size = 0\n",
    "  for x in params:\n",
    "    sz = 1\n",
    "    for dim in x.get_shape():\n",
    "      sz *= dim.value\n",
    "    size += sz\n",
    "  return size\n",
    "\n",
    "print('Model size is: ', model_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "display_freq = 200\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  sess.run(train.clear_char_embedding_padding)\n",
    "  prev_perplexity = float('inf')\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train.assign_lr(sess, learning_rate)\n",
    "    \n",
    "    iters = 0\n",
    "    costs = 0\n",
    "    \n",
    "    train_batches = batch_producer(train_raw_data, batch_size, num_steps)\n",
    "    training_state = None\n",
    "\n",
    "    for batch in train_batches:\n",
    "      my_x = np.empty([batch_size, num_steps, max_word_len], dtype=np.int32)\n",
    "\n",
    "      for t in range(num_steps):\n",
    "        for i in range(batch_size):\n",
    "          my_x[i, t] = word_ix_to_char_ixs(batch[0][i, t])\n",
    "\n",
    "      if not training_state: training_state = sess.run(train.init_state)\n",
    "      _, c, training_state, my_lr = sess.run([train.train_op, train.cost, train.state, train.lr],\n",
    "                                             feed_dict={train.x: my_x, train.y: batch[1], \n",
    "                                                        train.init_state: training_state})\n",
    "      sess.run(train.clear_char_embedding_padding)\n",
    "      \n",
    "      costs += c\n",
    "      if iters % (display_freq*num_steps) == 0 and iters != 0:\n",
    "        print('step =', iters/num_steps, end=', ')\n",
    "        print('perplexity =', np.exp(costs / iters), end=', ')\n",
    "        print('learning rate =', my_lr, end=', ')\n",
    "        print('speed =', round(iters * batch_size / (time.time() - start_time)), ' wps')\n",
    "\n",
    "      iters += num_steps\n",
    "    \n",
    "    print('epoch ', epoch+1, end = ': ')\n",
    "    print('perplexity =', np.exp(costs / iters), end=', ')\n",
    "    print('learning rate =', my_lr)\n",
    "    \n",
    "    \n",
    "    # Get validation set perplexity\n",
    "    valid_costs = 0\n",
    "    valid_state = None\n",
    "    valid_iters = 0\n",
    "\n",
    "    valid_batches = batch_producer(valid_raw_data, batch_size, num_steps)\n",
    "    \n",
    "    for valid_batch in valid_batches:\n",
    "      my_valid_x = np.empty([batch_size, num_steps, max_word_len], dtype=np.int32)\n",
    "      \n",
    "      for t in range(num_steps):\n",
    "        for i in range(batch_size):\n",
    "          my_valid_x[i, t] = word_ix_to_char_ixs(valid_batch[0][i, t])\n",
    "      \n",
    "      if not valid_state: valid_state = sess.run(valid.init_state)\n",
    "      c, valid_state = sess.run([valid.cost, valid.state], \n",
    "                                feed_dict={valid.x: my_valid_x, valid.y: valid_batch[1], \n",
    "                                           valid.init_state: valid_state})\n",
    "\n",
    "      valid_costs += c\n",
    "      valid_iters += num_steps\n",
    "    \n",
    "    cur_perplexity = np.exp(valid_costs / valid_iters)\n",
    "    print('Validation set perplexity =', cur_perplexity)\n",
    "\n",
    "    if prev_perplexity - cur_perplexity < 1:\n",
    "      learning_rate *= lr_decay\n",
    "    prev_perplexity = cur_perplexity\n",
    "\n",
    "    \n",
    "    # Get test set perplexity\n",
    "    test_costs = 0\n",
    "    test_state = None\n",
    "    test_iters = 0\n",
    "\n",
    "    test_batches = batch_producer(test_raw_data, 1, num_steps)\n",
    "    \n",
    "    for test_batch in test_batches:\n",
    "      my_test_x = np.empty([1, num_steps, max_word_len], dtype=np.int32)\n",
    "      \n",
    "      for t in range(num_steps):\n",
    "        for i in range(1):\n",
    "          my_test_x[i, t] = word_ix_to_char_ixs(test_batch[0][i, t])\n",
    "\n",
    "      if not test_state: test_state = sess.run(test.init_state)\n",
    "      c, test_state = sess.run([test.cost, test.state], \n",
    "                                feed_dict={test.x: my_test_x, test.y: test_batch[1], \n",
    "                                           test.init_state: test_state})\n",
    "\n",
    "      test_costs += c\n",
    "      test_iters += num_steps\n",
    "    \n",
    "    print('Test set perplexity =', np.exp(test_costs / test_iters))\n",
    "       \n",
    "\n",
    "    print('-' * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
